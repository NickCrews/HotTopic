notes:

4/2:
started a run on mcscn at 11:30 on augmented data, called convWithAugmentedBig.h5
8 per epoch, 128 epochs

started a run on mcscn at noon on augmented data, called convWithAugmentedBigger.h5
16 per epoch, 256 epochs

4/3:
5pm
made it so augmenting pads the starting layers so we don't crop useful stuff when rotating or scaling. now re-ran the preProcessor fit so that it fits on the new kind of augmented data, since it contains a lot more zeros now, so the mean and std dev are often different

4/4:
now we have 3 steps: augmenting, pre-processing to get data in final matrix, then normalization.

4/5:
finished the weather compatibility and trained for 64 epochs, batchSize=16. It predicts nothing ever burns.
I think this is because only ~5% of pixels actually did burn, so it just always say nothing burns


4/7:
Trying to fix this imbalance issue.

made a custom loss metric that heavily penalizes guessing 0 on a true-1 pixel, when the majority of pixels are 0. As could maybe be predicted if you thought about it (which I didnt lol) now it just thinks EVERYTHHING burnt because its terrified having a false negative.

4/8:
tried to go back to where we were in december, using patches as inputs, so that we can balance the pos and neg samples
merged some of the newer stuff, such as on-the-fly augmenting of days, and then normalizing only just before training.

normalizer saved as back2patches.json.
trained for 64 epochs, 16 batches per epoch, each batch 16 samples large. saved model as back2patches2 (back2patches was a test run for 4 epochs to make sure there wouldnt be crashes)

It is back to guessing around .5 for every pixel we feed it.

about to try to reset to branch from december. Have been terrified to do that so far because I wont be able to remember whats going on
